{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "loose-record",
   "metadata": {},
   "source": [
    "# GeoJSON 데이터 로드 ( 제대로 읽어지지 않는 방법과 제대로 읽는 방법 )\n",
    "\n",
    "https://medium.com/@sabman/loading-geojson-data-in-apache-spark-f7a52390cdc9   \n",
    "\n",
    "https://databricks.com/blog/2019/12/05/processing-geospatial-data-at-scale-with-databricks.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-detective",
   "metadata": {},
   "source": [
    "## 제대로 읽어지지 않는다\n",
    "https://medium.com/@sabman/loading-geojson-data-in-apache-spark-f7a52390cdc9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-fifteen",
   "metadata": {},
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    "  .builder()\n",
    "  .appName(\"GeoJson\")\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "editorial-bible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- properties: struct (nullable = true)\n",
      " |    |-- @id: string (nullable = true)\n",
      " |    |-- borough: string (nullable = true)\n",
      " |    |-- boroughCode: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "raw: org.apache.spark.sql.DataFrame = [_corrupt_record: string, geometry: struct<coordinates: array<array<array<double>>>, type: string> ... 3 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val raw = spark.read.json(\"hdfs://namenode:8020/taxidata/nyc-borough-boundaries-polygon.geojson\")\n",
    "raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fatty-copyright",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+--------------------+-------+\n",
      "|     _corrupt_record|            geometry|  id|          properties|   type|\n",
      "+--------------------+--------------------+----+--------------------+-------+\n",
      "|                   {|                null|null|                null|   null|\n",
      "|\"type\": \"FeatureC...|                null|null|                null|   null|\n",
      "|       \"features\": [|                null|null|                null|   null|\n",
      "|                null|[[[[-74.050508064...|   0|[http://nyc.pedia...|Feature|\n",
      "|                   ,|                null|null|                null|   null|\n",
      "|                null|[[[[-74.053140368...|   1|[http://nyc.pedia...|Feature|\n",
      "|                   ,|                null|null|                null|   null|\n",
      "|                null|[[[[-74.159456024...|   2|[http://nyc.pedia...|Feature|\n",
      "|                   ,|                null|null|                null|   null|\n",
      "|                null|[[[[-74.082212729...|   3|[http://nyc.pedia...|Feature|\n",
      "|                   ,|                null|null|                null|   null|\n",
      "|                null|[[[[-73.836682741...|   4|[http://nyc.pedia...|Feature|\n",
      "|                   ,|                null|null|                null|   null|\n",
      "|                null|[[[[-73.813396652...|   5|[http://nyc.pedia...|Feature|\n",
      "|                   ,|                null|null|                null|   null|\n",
      "|                null|[[[[-73.827182821...|   6|[http://nyc.pedia...|Feature|\n",
      "|                   ,|                null|null|                null|   null|\n",
      "|                null|[[[[-73.826074726...|   7|[http://nyc.pedia...|Feature|\n",
      "|                   ,|                null|null|                null|   null|\n",
      "|                null|[[[[-73.832442073...|   8|[http://nyc.pedia...|Feature|\n",
      "+--------------------+--------------------+----+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-experiment",
   "metadata": {},
   "source": [
    "파일이 깨져서 로드 되어서 약간 필터링이 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alleged-founder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------------------+-------+\n",
      "|            geometry| id|          properties|   type|\n",
      "+--------------------+---+--------------------+-------+\n",
      "|[[[[-74.050508064...|  0|[http://nyc.pedia...|Feature|\n",
      "|[[[[-74.053140368...|  1|[http://nyc.pedia...|Feature|\n",
      "|[[[[-74.159456024...|  2|[http://nyc.pedia...|Feature|\n",
      "|[[[[-74.082212729...|  3|[http://nyc.pedia...|Feature|\n",
      "|[[[[-73.836682741...|  4|[http://nyc.pedia...|Feature|\n",
      "|[[[[-73.813396652...|  5|[http://nyc.pedia...|Feature|\n",
      "|[[[[-73.827182821...|  6|[http://nyc.pedia...|Feature|\n",
      "|[[[[-73.826074726...|  7|[http://nyc.pedia...|Feature|\n",
      "|[[[[-73.832442073...|  8|[http://nyc.pedia...|Feature|\n",
      "|[[[[-73.794201726...|  9|[http://nyc.pedia...|Feature|\n",
      "|[[[[-73.805097201...| 10|[http://nyc.pedia...|Feature|\n",
      "|[[[[-73.804991988...| 11|[http://nyc.pedia...|Feature|\n",
      "|[[[[-73.739558564...| 12|[http://nyc.pedia...|Feature|\n",
      "|[[[[-73.739443808...| 13|[http://nyc.pedia...|Feature|\n",
      "|[[[[-73.790549485...| 14|[http://nyc.pedia...|Feature|\n",
      "|[[[[-73.766708277...| 15|[http://nyc.pedia...|Feature|\n",
      "|[[[[-73.797835333...| 16|[http://nyc.pedia...|Feature|\n",
      "|[[[[-73.813072338...| 17|[http://nyc.pedia...|Feature|\n",
      "|[[[[-73.823375921...| 18|[http://nyc.pedia...|Feature|\n",
      "|[[[[-73.858980029...| 19|[http://nyc.pedia...|Feature|\n",
      "+--------------------+---+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "filteredDF: org.apache.spark.sql.DataFrame = [geometry: struct<coordinates: array<array<array<double>>>, type: string>, id: bigint ... 2 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filteredDF = raw.drop(\"_corrupt_record\").na.drop()\n",
    "filteredDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baking-profile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+-------------+-----------+\n",
      "|@id                                                      |borough      |boroughCode|\n",
      "+---------------------------------------------------------+-------------+-----------+\n",
      "|http://nyc.pediacities.com/Resource/Borough/Staten_Island|Staten Island|5          |\n",
      "|http://nyc.pediacities.com/Resource/Borough/Staten_Island|Staten Island|5          |\n",
      "|http://nyc.pediacities.com/Resource/Borough/Staten_Island|Staten Island|5          |\n",
      "|http://nyc.pediacities.com/Resource/Borough/Staten_Island|Staten Island|5          |\n",
      "|http://nyc.pediacities.com/Resource/Borough/Queens       |Queens       |4          |\n",
      "|http://nyc.pediacities.com/Resource/Borough/Queens       |Queens       |4          |\n",
      "|http://nyc.pediacities.com/Resource/Borough/Queens       |Queens       |4          |\n",
      "|http://nyc.pediacities.com/Resource/Borough/Queens       |Queens       |4          |\n",
      "|http://nyc.pediacities.com/Resource/Borough/Queens       |Queens       |4          |\n",
      "|http://nyc.pediacities.com/Resource/Borough/Queens       |Queens       |4          |\n",
      "|http://nyc.pediacities.com/Resource/Borough/Queens       |Queens       |4          |\n",
      "|http://nyc.pediacities.com/Resource/Borough/Queens       |Queens       |4          |\n",
      "|http://nyc.pediacities.com/Resource/Borough/Queens       |Queens       |4          |\n",
      "|http://nyc.pediacities.com/Resource/Borough/Queens       |Queens       |4          |\n",
      "|http://nyc.pediacities.com/Resource/Borough/Queens       |Queens       |4          |\n",
      "|http://nyc.pediacities.com/Resource/Borough/Queens       |Queens       |4          |\n",
      "|http://nyc.pediacities.com/Resource/Borough/Queens       |Queens       |4          |\n",
      "|http://nyc.pediacities.com/Resource/Borough/Queens       |Queens       |4          |\n",
      "|http://nyc.pediacities.com/Resource/Borough/Queens       |Queens       |4          |\n",
      "|http://nyc.pediacities.com/Resource/Borough/Queens       |Queens       |4          |\n",
      "+---------------------------------------------------------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "properties: org.apache.spark.sql.DataFrame = [@id: string, borough: string ... 1 more field]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val properties = filteredDF.select(col(\"properties.@id\"), col(\"properties.borough\"), col(\"properties.boroughCode\"))\n",
    "properties.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-process",
   "metadata": {},
   "source": [
    "## 제대로 읽어 보자 GeoJson\n",
    "https://databricks.com/blog/2019/12/05/processing-geospatial-data-at-scale-with-databricks.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-indiana",
   "metadata": {},
   "source": [
    "기본적으로 스파크는 JSON 파일의 모든 레코드가 한 라인으로 완벽하다고 가정한다. 그렇지 않은 데이터에 대해 멀티라인이라는 옵션을 줄 수가 있다.\n",
    "~~~scala \n",
    "option(\"multiline\", \"false\") // Default\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "strategic-prison",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_json_df: org.apache.spark.sql.DataFrame = [features: array<struct<geometry:struct<coordinates:array<array<array<double>>>,type:string>,id:bigint,properties:struct<@id:string,borough:string,boroughCode:bigint>,type:string>>, type: string]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val raw_json_df = spark.read.option(\"multiline\", \"true\").json(\"hdfs://namenode:8020/taxidata/nyc-borough-boundaries-polygon.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "conditional-seller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- geometry: struct (nullable = true)\n",
      " |    |    |    |-- coordinates: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- properties: struct (nullable = true)\n",
      " |    |    |    |-- @id: string (nullable = true)\n",
      " |    |    |    |-- borough: string (nullable = true)\n",
      " |    |    |    |-- boroughCode: long (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abroad-haven",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "skilled-wedding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.explode\n",
       "explode_properties: org.apache.spark.sql.DataFrame = [features: array<struct<geometry:struct<coordinates:array<array<array<double>>>,type:string>,id:bigint,properties:struct<@id:string,borough:string,boroughCode:bigint>,type:string>>, type: string ... 1 more field]\n",
       "explode_geometry: org.apache.spark.sql.DataFrame = [features: array<struct<geometry:struct<coordinates:array<array<array<double>>>,type:string>,id:bigint,properties:struct<@id:string,borough:string,boroughCode:bigint>,type:string>>, type: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.explode\n",
    "\n",
    "val explode_properties = raw_json_df.withColumn(\"properties\", explode(col(\"features.properties\")))\n",
    "val explode_geometry = explode_properties.withColumn(\"geometry\", explode(col(\"features.geometry\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "placed-parcel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- properties: struct (nullable = true)\n",
      " |    |-- @id: string (nullable = true)\n",
      " |    |-- borough: string (nullable = true)\n",
      " |    |-- boroughCode: long (nullable = true)\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |-- type: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "parsed: org.apache.spark.sql.DataFrame = [type: string, properties: struct<@id: string, borough: string ... 1 more field> ... 1 more field]\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parsed = explode_geometry.drop(col(\"features\"))\n",
    "parsed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "looking-affiliate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: Long = 416\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.filter(col(\"properties.@id\") === \"http://nyc.pediacities.com/Resource/Borough/Staten_Island\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ceramic-flavor",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "42: error: overloaded method value select with alternatives:",
     "output_type": "error",
     "traceback": [
      "<console>:42: error: overloaded method value select with alternatives:",
      "  [U1, U2](c1: org.apache.spark.sql.TypedColumn[org.apache.spark.sql.Row,U1], c2: org.apache.spark.sql.TypedColumn[org.apache.spark.sql.Row,U2])org.apache.spark.sql.Dataset[(U1, U2)] <and>",
      "  (col: String,cols: String*)org.apache.spark.sql.DataFrame <and>",
      "  (cols: org.apache.spark.sql.Column*)org.apache.spark.sql.DataFrame",
      " cannot be applied to (String, org.apache.spark.sql.Column)",
      "       raw_json_df.select(\"features\", explode(col(\"features\")))",
      "                   ^",
      ""
     ]
    }
   ],
   "source": [
    "raw_json_df.select(\"features\", explode(col(\"features\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-seeking",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
