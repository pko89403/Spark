{"cells":[{"cell_type":"markdown","source":["# Spark 에서 Pytorch로 간단하게 데이터 변환하기\n1. 스파크를 사용해서 데이터 로드\n2. 스파크 데이터프레임을 petastorm spark_dataset_converter를 사용해서 Pytorch 데이터 로더로 변환\n3. 학습을 위해 싱글 노드 파이토치 모델에 데이터를 피딩\n4. 분산 하이퍼 파라미터 튜닝 함수에 데이터를 피딩\n5. 파이토치 분산 모델에 데이터 피딩"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ed87eda-3c48-4a62-aaaf-f3061d5d9fab"}}},{"cell_type":"code","source":["import petastorm \nimport torch\nimport pyarrow\n\nprint(\"petastorm 버전 : \", petastorm.__version__)\nprint(\"pyarrow 버전 : \", pyarrow.__version__)\nprint(\"pytorch 버전 : \", torch.__version__)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1a0c044c-b344-4344-b547-710b15776600"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">petastorm 버전 :  0.9.7\npyarrow 버전 :  1.0.1\npytorch 버전 :  1.7.0\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">petastorm 버전 :  0.9.7\npyarrow 버전 :  1.0.1\npytorch 버전 :  1.7.0\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\nfrom petastorm.spark import SparkDatasetConverter, make_spark_converter\n\nimport io\nimport numpy as np\nimport torch\nimport torchvision\nfrom PIL import Image\nfrom functools import partial \nfrom petastorm import TransformSpec\nfrom torchvision import transforms\n\nfrom hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK\n\nimport horovod.torch as hvd\nfrom sparkdl import HorovodRunner"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e13a5f5-dcba-4389-8bb0-2559ac50ce32"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["BATCH_SIZE = 32\nNUM_EPOCHS = 10"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51468436-d198-4997-bf51-ecda51dbce23"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# flowers 데이터 셋을 사용한다. ( 이미지 분류 )\n# 빠른 예제 테스트를 위해서 데이터를 총 100개만 사용함.\ndf = spark.read.format(\"delta\").load(\"/databricks-datasets/flowers/delta\") \\\n  .select(col(\"content\"), col(\"label_index\")) \\\n  .limit(1000)\n  \nnum_classes = df.select(\"label_index\").distinct().count()\nprint(\"분류 클래스 개수 - \", num_classes)\n# 학습 데이터로 이미지 90개 검증 데이터로 이미지 10개를 사용함.\ndf_train, df_val = df.randomSplit([0.9, 0.1], seed=12345)\n\n# Make sure the number of partitions is at least the number of workers which is required for distributed training.\ndf_train = df_train.repartition(2)\ndf_val = df_val.repartition(2)\n\n\nprint(f\"학습 데이터 - {df_train.count()},\\t 검증 데이터 - {df_val.count()}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0fa48d2b-dbb8-469a-b1ee-165de1a54c67"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">분류 클래스 개수 -  5\n학습 데이터 - 919,\t 검증 데이터 - 81\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">분류 클래스 개수 -  5\n학습 데이터 - 919,\t 검증 데이터 - 81\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# petastorm의 make_spark_converter를 사용해서 데이터프레임을 캐시한다. \n# import petatorm.spark.make_spark_converter\n# 데이터브릭스에서 제공하는 파일 시스템에 스파크 데이터프레임을 캐시한다.\nspark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, \"file:///dbfs/tmp/petastorm/cache\")\n\nconverter_train = make_spark_converter(df_train)\nconverter_val = make_spark_converter(df_val)\n\nprint(f\"학습 데이터 : {len(converter_train)}, 검증 데이터 : {len(converter_val)}\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3b0607d-cf5d-4fd0-83d7-2d54cd8dc9ac"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Converting floating-point columns to float32\nThe median size 34210620 B (&lt; 50 MB) of the parquet files is too small. Total size: 67531224 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:/dbfs/tmp/petastorm/cache/20210321061903-appid-local-1616299138888-5d015866-031f-428b-acf2-38c3c10db92d/part-00001-tid-4341555411260954623-a9c39ff1-aef7-49e0-8d6b-43231b65c0c8-57-1-c000.parquet, ...\nConverting floating-point columns to float32\nThe median size 3356722 B (&lt; 50 MB) of the parquet files is too small. Total size: 6257076 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:/dbfs/tmp/petastorm/cache/20210321061911-appid-local-1616299138888-5164c6c9-2304-462f-8fb6-552c0a9e791d/part-00001-tid-1878163162042784096-14f955c3-d4bf-4658-b928-581a008ad7a0-66-1-c000.parquet, ...\n학습 데이터 : 919, 검증 데이터 : 81\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Converting floating-point columns to float32\nThe median size 34210620 B (&lt; 50 MB) of the parquet files is too small. Total size: 67531224 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:/dbfs/tmp/petastorm/cache/20210321061903-appid-local-1616299138888-5d015866-031f-428b-acf2-38c3c10db92d/part-00001-tid-4341555411260954623-a9c39ff1-aef7-49e0-8d6b-43231b65c0c8-57-1-c000.parquet, ...\nConverting floating-point columns to float32\nThe median size 3356722 B (&lt; 50 MB) of the parquet files is too small. Total size: 6257076 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:/dbfs/tmp/petastorm/cache/20210321061911-appid-local-1616299138888-5164c6c9-2304-462f-8fb6-552c0a9e791d/part-00001-tid-1878163162042784096-14f955c3-d4bf-4658-b928-581a008ad7a0-66-1-c000.parquet, ...\n학습 데이터 : 919, 검증 데이터 : 81\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["'''\ndef get_model(lr=1e-2):\n  # torchvision 라이브러리에서 MobileNetV2 모델을 가져온다.\n  model = torchvision.models.resnet18(pretrained=True)\n  # 모델 파라미터를 학습하지 않도록 만든다.\n  for param in model.parameters():\n    param.requires_gra = False\n  \n  # 트랜스퍼 러닝을 위해서 분류 목적의 레이어를 추가함\n  num_ftrs = model.fc.in_features\n\n  # 새롭게 추가되는 레이어는 학습이 가능하다. ( requires_grad=True)\n  model.fc = torch.nn.Linear(num_ftrs, num_classes)\n  \n  return model\n'''"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51780ea4-f7e9-4ae9-baf1-7018182a1e46"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[6]: &#39;\\ndef get_model(lr=1e-2):\\n  # torchvision 라이브러리에서 MobileNetV2 모델을 가져온다.\\n  model = torchvision.models.resnet18(pretrained=True)\\n  # 모델 파라미터를 학습하지 않도록 만든다.\\n  for param in model.parameters():\\n    param.requires_gra = False\\n  \\n  # 트랜스퍼 러닝을 위해서 분류 목적의 레이어를 추가함\\n  num_ftrs = model.fc.in_features\\n\\n  # 새롭게 추가되는 레이어는 학습이 가능하다. ( requires_grad=True)\\n  model.fc = torch.nn.Linear(num_ftrs, num_classes)\\n  \\n  return model\\n&#39;</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[6]: &#39;\\ndef get_model(lr=1e-2):\\n  # torchvision 라이브러리에서 MobileNetV2 모델을 가져온다.\\n  model = torchvision.models.resnet18(pretrained=True)\\n  # 모델 파라미터를 학습하지 않도록 만든다.\\n  for param in model.parameters():\\n    param.requires_gra = False\\n  \\n  # 트랜스퍼 러닝을 위해서 분류 목적의 레이어를 추가함\\n  num_ftrs = model.fc.in_features\\n\\n  # 새롭게 추가되는 레이어는 학습이 가능하다. ( requires_grad=True)\\n  model.fc = torch.nn.Linear(num_ftrs, num_classes)\\n  \\n  return model\\n&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import torch.nn as nn\nimport torch.nn.functional as F \n\nclass Net(nn.Module):\n  def __init__(self):\n    super(Net, self).__init__()\n    self.conv1 = nn.Conv2d(3, 3, 1)\n    self.pool = nn.MaxPool2d(2, 2) # \n    self.fc1 = nn.Linear(3*112*112, 5)\n  \n  def forward(self, x):\n    x = self.pool(F.relu(self.conv1(x))) #224 - 1 + 1 = 224 -> 112\n    x = x.view(-1, 3*112*112)\n    x = self.fc1(x)\n    return x\n  \ndef get_model():\n  return Net()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05a0523a-fe63-4d74-b6b9-56fd303eef1f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def train_one_epoch(model, criterion, optimizer, scheduler,\n                    train_dataloader_iter, steps_per_epoch, epoch,\n                    device):\n  # 모델을 학습 모드로 만든다.\n  model.train()\n  \n  running_loss = 0.0\n  running_corrects = 0\n  \n  # 1 epoch 마다 데이터에 대해 iteration\n  for step in range(steps_per_epoch):\n    pd_batch = next(train_dataloader_iter)\n    \n    inputs, labels = pd_batch['features'].to(device), pd_batch['label_index'].to(device)\n    \n    with torch.set_grad_enabled(True):\n      optimizer.zero_grad()\n      \n      # Forward -모델에 데이터를 Input한다.\n      outputs = model(inputs)\n      _, preds = torch.max(outputs, 1)\n      loss = criterion(outputs, labels)\n      \n      # Backward + Optimize - Loss를 역전파 시키고 모델 파라미터를 업데이트 한다.\n      loss.backward()\n      optimizer.step()\n  \n    running_loss += loss.item() * inputs.size(0)\n    running_corrects += torch.sum(preds == labels.data)\n  \n  scheduler.step()\n  \n  epoch_loss = running_loss / (steps_per_epoch * BATCH_SIZE)\n  epoch_acc = running_corrects.double() / ( steps_per_epoch * BATCH_SIZE )\n\n  print(\"학습 Loss : {:.4f} 정확도 : {:.4f}\".format(epoch_loss, epoch_acc))\n  return epoch_loss, epoch_acc"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8133674b-2b0a-4d07-bc76-6ddf1e8fe7f5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def evaluate(model, criterion, \n             val_dataloader_iter, validation_steps,\n             device, metric_agg_fn=None):\n  # 모델을 evaluate 모드로 만듬\n  model.eval()\n  \n  running_loss = 0.0\n  running_corrects = 0\n  \n  # 모든 validation set에 대해 Iterate 한다.\n  for step in range(validation_steps):\n    pd_batch = next(val_dataloader_iter)\n    inputs, labels = pd_batch['features'].to(device), pd_batch['label_index'].to(device)\n    \n    with torch.set_grad_enabled(False):\n      # Forward\n      outputs = model(inputs)\n      _, preds = torch.max(outputs, 1)\n      loss = criterion(outputs, labels)\n      \n    running_loss += loss.item()\n    running_corrects += torch.sum(preds == labels.data)\n    \n  epoch_loss = running_loss / validation_steps\n  epoch_acc = running_corrects.double() / (validation_steps * BATCH_SIZE)\n  \n  if metric_agg_fn != None:\n    epoch_loss = metric_agg_fn(epoch_loss, 'avg_loss')\n    epoch_acc = metric_agg_fn(epoch_acc, 'avg_acc')\n    \n  print(\"검증 Loss : {:.4f} 정확도 : {:.4f}\".format(epoch_loss, epoch_acc))\n  return epoch_loss, epoch_acc"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"72e04325-5dea-486a-98fa-13cfd401c2aa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 이미지 처리\n모델에 데이터셋을 피딩하기 전, 바이너리 형태로 되어있는 이미지 데이터를 변환한다.     \n데이터브릭스에서는 스파크 데이터프레임을 사용해서 이미지로 변환하는 것은 권장하지 않음.     \n이미지로 변환하면서 사이즈가 엄청 커지기 때문에 성능 감소로 이어질 수 있다고 한다.    \n대신에 petastorm에서 변환을 진행을 하는 것을 권장함"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"972546c9-88a0-40f4-877d-5c92b7587acf"}}},{"cell_type":"code","source":["def transform_row(is_train, pd_batch):\n  # 입력과 아웃풋이 반드시 pandas.DataFrame 이어야 한다.\n  transformers = [transforms.Lambda(lambda x: Image.open(io.BytesIO(x)))]\n  \n  if is_train:\n    # 학습 데이터에 Data Augmentation을 한다.\n    transformers.extend([\n      transforms.RandomResizedCrop(224),\n      transforms.RandomHorizontalFlip(),\n    ])\n  else:\n    transformers.extend([\n      transforms.Resize(256),\n      transforms.CenterCrop(224),\n    ])\n  \n  transformers.extend([\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n  ])\n  \n  trans = transforms.Compose(transformers)\n  \n  pd_batch['features'] = pd_batch['content'].map(lambda x: trans(x).numpy())\n  pd_batch = pd_batch.drop(labels=['content'], axis=1)\n\n  return pd_batch"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c18e3e3-a6b4-4b37-a226-6ed2250e4dd9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_transform_spec(is_train=True):\n  # TransformSpec 의 output shape는 petastorm에 자동으로 전달되는 것이 아니다.\n  # edit_fields 에서 새로운 column의 shape를 명시해줘야하고 \n  # selected_fields 에 output column의 순서를 명시해줘야한다.\n  return TransformSpec(partial(transform_row, is_train),\n                       edit_fields=[('features', np.float32, (3, 224, 224), False)],\n                       selected_fields=['features', 'label_index'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b19211ee-c9fa-4539-b59e-587e4624d9f1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def train_and_evaluate(lr=1e-3):\n  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n  model = get_model()\n  model = model.to(device)\n  \n  criterion = torch.nn.CrossEntropyLoss()\n  \n  # 마지막 분류 레이어만 학습한다.\n  optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n  \n  # Learning Rate를 7 epoch 마다 0.1 씩 감소시킨다.\n  exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n  \n  # converter.make_torch_dataloader 함수를 사용해서 파이토치 데이터로드를 생성한다.\n  # converter_train = make_spark_converter(df_train)\n  with converter_train.make_torch_dataloader(transform_spec = get_transform_spec(is_train=True),\n                                             batch_size = BATCH_SIZE) as train_dataloader, \\\n       converter_val.make_torch_dataloader(transform_spec = get_transform_spec(is_train=False),\n                                             batch_size = BATCH_SIZE) as val_dataloader:\n    \n    train_dataloader_iter = iter(train_dataloader)\n    steps_per_epoch = len(converter_train) // BATCH_SIZE\n    \n    val_dataloader_iter = iter(val_dataloader)\n    validation_steps = max(1, len(converter_val)) // BATCH_SIZE\n                           \n    for epoch in range(NUM_EPOCHS):\n      print('Epoch {}/{}'.format(epoch+1, NUM_EPOCHS))\n      print('_' * 10)\n                           \n      train_loss, train_acc = train_one_epoch(model, criterion, optimizer, exp_lr_scheduler,\n                                              train_dataloader_iter, steps_per_epoch, epoch,\n                                              device)\n      val_loss, val_acc = evaluate(model, criterion, val_dataloader_iter, validation_steps, device)\n\n  return val_loss  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7813702-d7e8-4975-99bc-ea076286b807"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["loss = train_and_evaluate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"648ec772-ecd2-4645-84f2-5f869701bdb5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Epoch 1/10\n__________\n학습 Loss : 1.7129 정확도 : 0.2656\n검증 Loss : 1.3634 정확도 : 0.3906\nEpoch 2/10\n__________\n학습 Loss : 1.4331 정확도 : 0.3493\n검증 Loss : 1.2927 정확도 : 0.3906\nEpoch 3/10\n__________\n학습 Loss : 1.3969 정확도 : 0.3817\n검증 Loss : 1.4104 정확도 : 0.4375\nEpoch 4/10\n__________\n학습 Loss : 1.3215 정확도 : 0.4364\n검증 Loss : 1.2139 정확도 : 0.5312\nEpoch 5/10\n__________\n학습 Loss : 1.2968 정확도 : 0.4196\n검증 Loss : 1.2444 정확도 : 0.5000\nEpoch 6/10\n__________\n학습 Loss : 1.2485 정확도 : 0.4632\n검증 Loss : 1.2224 정확도 : 0.5469\nEpoch 7/10\n__________\n학습 Loss : 1.2544 정확도 : 0.4621\n검증 Loss : 1.2156 정확도 : 0.5312\nEpoch 8/10\n__________\n학습 Loss : 1.2236 정확도 : 0.4877\n검증 Loss : 1.0583 정확도 : 0.6250\nEpoch 9/10\n__________\n학습 Loss : 1.2147 정확도 : 0.4788\n검증 Loss : 1.2498 정확도 : 0.5156\nEpoch 10/10\n__________\n학습 Loss : 1.1909 정확도 : 0.5033\n검증 Loss : 1.2267 정확도 : 0.5000\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Epoch 1/10\n__________\n학습 Loss : 1.7129 정확도 : 0.2656\n검증 Loss : 1.3634 정확도 : 0.3906\nEpoch 2/10\n__________\n학습 Loss : 1.4331 정확도 : 0.3493\n검증 Loss : 1.2927 정확도 : 0.3906\nEpoch 3/10\n__________\n학습 Loss : 1.3969 정확도 : 0.3817\n검증 Loss : 1.4104 정확도 : 0.4375\nEpoch 4/10\n__________\n학습 Loss : 1.3215 정확도 : 0.4364\n검증 Loss : 1.2139 정확도 : 0.5312\nEpoch 5/10\n__________\n학습 Loss : 1.2968 정확도 : 0.4196\n검증 Loss : 1.2444 정확도 : 0.5000\nEpoch 6/10\n__________\n학습 Loss : 1.2485 정확도 : 0.4632\n검증 Loss : 1.2224 정확도 : 0.5469\nEpoch 7/10\n__________\n학습 Loss : 1.2544 정확도 : 0.4621\n검증 Loss : 1.2156 정확도 : 0.5312\nEpoch 8/10\n__________\n학습 Loss : 1.2236 정확도 : 0.4877\n검증 Loss : 1.0583 정확도 : 0.6250\nEpoch 9/10\n__________\n학습 Loss : 1.2147 정확도 : 0.4788\n검증 Loss : 1.2498 정확도 : 0.5156\nEpoch 10/10\n__________\n학습 Loss : 1.1909 정확도 : 0.5033\n검증 Loss : 1.2267 정확도 : 0.5000\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 4. 분산 하이퍼 파라미터 튜닝 함수에 데이터 피딩\nHyperopt 라이브러리를 사용해서 하이퍼파라미터 튜닝 작업을 한다."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8f946541-6655-4ff7-a31c-738fcb99a3fc"}}},{"cell_type":"code","source":["import hyperopt \nimport mlflow \n\ndef train_fn(lr):\n  loss = train_and_evaluate(lr)\n  return { 'loss' : loss, 'status' : hyperopt.STATUS_OK }\n\nsearch_space = hp.loguniform('lr', -10, -4)\nwith mlflow.start_run():\n  argmin = fmin(\n    fn = train_fn,\n    space = search_space, \n    algo = hyperopt.rand.suggest,\n    max_evals = 2,\n    trials = SparkTrials(parallelism = 2))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54ef8b7f-3ee2-435c-8c53-5673ed62496f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Hyperopt with SparkTrials will automatically track trials in MLflow. To view the MLflow experiment associated with the notebook, click the &#39;Runs&#39; icon in the notebook context bar on the upper right. There, you can view all runs.\nTo view logs from trials, please check the Spark executor logs. To view executor logs, expand &#39;Spark Jobs&#39; above until you see the (i) icon next to the stage from the trial job. Click it and find the list of tasks. Click the &#39;stderr&#39; link for a task to view trial logs.\n\r  0%|          | 0/2 [00:00&lt;?, ?trial/s, best loss=?]\r 50%|█████     | 1/2 [05:39&lt;05:39, 339.46s/trial, best loss: 1.2334869503974915]\r100%|██████████| 2/2 [05:40&lt;00:00, 237.92s/trial, best loss: 1.2334869503974915]\r100%|██████████| 2/2 [05:40&lt;00:00, 170.23s/trial, best loss: 1.2334869503974915]\nTotal Trials: 2: 2 succeeded, 0 failed, 0 cancelled.\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Hyperopt with SparkTrials will automatically track trials in MLflow. To view the MLflow experiment associated with the notebook, click the &#39;Runs&#39; icon in the notebook context bar on the upper right. There, you can view all runs.\nTo view logs from trials, please check the Spark executor logs. To view executor logs, expand &#39;Spark Jobs&#39; above until you see the (i) icon next to the stage from the trial job. Click it and find the list of tasks. Click the &#39;stderr&#39; link for a task to view trial logs.\n\r  0%|          | 0/2 [00:00&lt;?, ?trial/s, best loss=?]\r 50%|█████     | 1/2 [05:39&lt;05:39, 339.46s/trial, best loss: 1.2334869503974915]\r100%|██████████| 2/2 [05:40&lt;00:00, 237.92s/trial, best loss: 1.2334869503974915]\r100%|██████████| 2/2 [05:40&lt;00:00, 170.23s/trial, best loss: 1.2334869503974915]\nTotal Trials: 2: 2 succeeded, 0 failed, 0 cancelled.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# 최적화된 Learning Rate ( 하이퍼 파라미터 )\nargmin"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ad2b2041-954d-47c8-8ef2-c075857e94d1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[64]: {&#39;lr&#39;: 0.0005587506783898631}</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[64]: {&#39;lr&#39;: 0.0005587506783898631}</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 5. 분산 파이토치 모델\nHorovodRunner를 사용해서 분산 학습을 한다."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b31f368-1172-4894-9df1-8ed465250ae5"}}},{"cell_type":"code","source":["def metric_average(val, name):\n  tensor = torch.tensor(val)\n  avg_tensor = hvd.allreduce(tensor, name=name)\n  return avg_tensor.item()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99bd9e6f-acc1-40ed-ba4e-b8ba98f32444"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def train_and_evaluate_hvd(lr=1e-2):\n  # Horovod를 초기화한다.\n  hvd.init() \n  \n  if torch.cuda.is_available():\n    torch.cuda.set_device(hvd.local_rank())\n    device = torch.cuda.current_device()\n  else:\n    device = torch.device(\"cpu\")\n\n  model = get_model()\n  model = model.to(device)\n  \n  criterion = torch.nn.CrossEntropyLoss()\n  # 동기 분산 학습에서는 워커 수에 맞춰서 배치 사이즈가 효율적으로 조정된다.\n  # 증가 될 배치 사이즈에 따라서 학습율을 키워준다.\n  optimizer = torch.optim.SGD(model.parameters(), lr=lr * hvd.size(), momentum=0.9)\n  \n  \n\n  # 모든 워커가 동일한 파라미터를 가지고 학습할 수 있도록 모델 metadata와 optimizer를 broadcast 해준다.\n  hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n  hvd.broadcast_optimizer_state(optimizer, root_rank=0)\n  \n\n  # Horovod의 DistributedOptimizer로 optimizer를 래핑해준다.\n  optimizer_hvd = hvd.DistributedOptimizer(optimizer, named_parameters = model.named_parameters())\n  \n  exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_hvd, step_size = 7, gamma = 0.1)\n  \n  with converter_train.make_torch_dataloader(transform_spec = get_transform_spec(is_train=True),\n                                             cur_shard = hvd.rank(), \n                                             shard_count = hvd.size(),\n                                             batch_size = BATCH_SIZE) as train_dataloader, \\\n       converter_val.make_torch_dataloader(transform_spec = get_transform_spec(is_train=False),\n                                           cur_shard = hvd.rank(),\n                                           shard_count = hvd.size(),\n                                           batch_size = BATCH_SIZE) as val_dataloader:\n    \n    train_dataloader_iter = iter(train_dataloader)\n    steps_per_epoch = len(converter_train) // ( BATCH_SIZE * hvd.size())\n    \n    val_dataloader_iter = iter(val_dataloader)\n    validation_steps = max(1, len(converter_val) // ( BATCH_SIZE * hvd.size()))\n    \n    for epoch in range(NUM_EPOCHS):\n      print(\"Epoch {}/{}\".format(epoch + 1, NUM_EPOCHS))\n      print('-' * 10)\n      \n      train_loss, train_acc = train_one_epoch(model, criterion, optimizer_hvd, exp_lr_scheduler,\n                                              train_dataloader_iter, steps_per_epoch, epoch,\n                                              device)\n      val_loss, val_acc = evaluate(model, criterion, \n                                   val_dataloader_iter, validation_steps,\n                                   device, metric_agg_fn=metric_average)\n\n  return val_loss"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7ee440c-a504-440f-a750-d2b48fd40dc0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# 클러스터가 두개의 워커로 구성되어 있다고 가정한다.\nhr = HorovodRunner(np=2)\nhr.run(train_and_evaluate_hvd)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b4b5700-a3dc-4dfc-a280-c49cae736edc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">HorovodRunner will stream all training logs to notebook cell output. If there are too many logs, you\ncan adjust the log level in your train method. Or you can set driver_log_verbosity to\n&#39;log_callback_only&#39; and use a HorovodRunner log  callback on the first worker to get concise\nprogress updates.\nThe global names read or written to by the pickled function are {&#39;range&#39;, &#39;converter_val&#39;, &#39;hvd&#39;, &#39;NUM_EPOCHS&#39;, &#39;evaluate&#39;, &#39;get_transform_spec&#39;, &#39;iter&#39;, &#39;len&#39;, &#39;converter_train&#39;, &#39;torch&#39;, &#39;get_model&#39;, &#39;BATCH_SIZE&#39;, &#39;print&#39;, &#39;metric_average&#39;, &#39;max&#39;, &#39;train_one_epoch&#39;}.\nThe pickled object size is 7863 bytes.\n\n### How to enable Horovod Timeline? ###\nHorovodRunner has the ability to record the timeline of its activity with Horovod  Timeline. To\nrecord a Horovod Timeline, set the `HOROVOD_TIMELINE` environment variable  to the location of the\ntimeline file to be created. You can then open the timeline file  using the chrome://tracing\nfacility of the Chrome browser.\n\nStart training.\n[1,0]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,0]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,1]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,1]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,0]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,0]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,0]&lt;stdout&gt;:Epoch 1/10[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:----------[1,0]&lt;stdout&gt;:\n[1,1]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,1]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,1]&lt;stdout&gt;:Epoch 1/10\n[1,1]&lt;stdout&gt;:----------\n[1,1]&lt;stderr&gt;:/databricks/python/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py:53: FutureWarning: Calling .data on ChunkedArray is provided for compatibility after Column was removed, simply drop this attribute\n[1,1]&lt;stderr&gt;:  column_as_pandas = column.data.chunks[0].to_pandas()\n[1,0]&lt;stderr&gt;:/databricks/python/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py:53: FutureWarning: Calling .data on ChunkedArray is provided for compatibility after Column was removed, simply drop this attribute\n[1,0]&lt;stderr&gt;:  column_as_pandas = column.data.chunks[0].to_pandas()\n[1,0]&lt;stdout&gt;:학습 Loss : 3.0049 정확도 : 0.2991\n[1,1]&lt;stdout&gt;:학습 Loss : 2.9046 정확도 : 0.4464\n[1,1]&lt;stderr&gt;:-c:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n[1,0]&lt;stderr&gt;:-c:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n[1,1]&lt;stdout&gt;:검증 Loss : 1.4322 정확도 : 0.3438\n[1,0]&lt;stdout&gt;:검증 Loss : 1.4322 정확도 : 0.3438\n[1,0]&lt;stdout&gt;:Epoch 2/10\n[1,0]&lt;stdout&gt;:----------\n[1,1]&lt;stdout&gt;:Epoch 2/10\n[1,1]&lt;stdout&gt;:----------\n[1,0]&lt;stdout&gt;:학습 Loss : 1.4913 정확도 : 0.2991[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:학습 Loss : 1.2598 정확도 : 0.5268\n[1,1]&lt;stdout&gt;:검증 Loss : 1.3130 정확도 : 0.3906\n[1,1]&lt;stdout&gt;:Epoch 3/10\n[1,1]&lt;stdout&gt;:----------\n[1,0]&lt;stdout&gt;:검증 Loss : 1.3130 정확도 : 0.3906[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 3/10[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:----------[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:학습 Loss : 1.1122 정확도 : 0.6406\n[1,0]&lt;stdout&gt;:학습 Loss : 1.5724 정확도 : 0.2902\n[1,1]&lt;stdout&gt;:검증 Loss : 1.5220 정확도 : 0.3750\n[1,1]&lt;stdout&gt;:Epoch 4/10\n[1,1]&lt;stdout&gt;:----------\n[1,0]&lt;stdout&gt;:검증 Loss : 1.5220 정확도 : 0.3750[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 4/10[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:----------[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:학습 Loss : 1.6128 정확도 : 0.3080\n[1,1]&lt;stdout&gt;:학습 Loss : 0.9855 정확도 : 0.6183\n[1,0]&lt;stdout&gt;:검증 Loss : 1.4703 정확도 : 0.3438\n[1,0]&lt;stdout&gt;:Epoch 5/10\n[1,0]&lt;stdout&gt;:----------[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:검증 Loss : 1.4703 정확도 : 0.3438\n[1,1]&lt;stdout&gt;:Epoch 5/10\n[1,1]&lt;stdout&gt;:----------[1,1]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:학습 Loss : 1.1860 정확도 : 0.6228\n[1,0]&lt;stdout&gt;:학습 Loss : 1.5704 정확도 : 0.2969[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:검증 Loss : 1.4714 정확도 : 0.3750\n[1,1]&lt;stdout&gt;:Epoch 6/10\n[1,1]&lt;stdout&gt;:----------\n[1,0]&lt;stdout&gt;:검증 Loss : 1.4714 정확도 : 0.3750[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 6/10[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:----------[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:학습 Loss : 1.2572 정확도 : 0.6205[1,1]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:학습 Loss : 1.5042 정확도 : 0.2835\n[1,0]&lt;stdout&gt;:검증 Loss : 1.4258 정확도 : 0.3750[1,1]&lt;stdout&gt;:검증 Loss : 1.4258 정확도 : 0.3750[1,1]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:Epoch 7/10\n[1,1]&lt;stdout&gt;:----------\n[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 7/10[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:----------[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:학습 Loss : 1.1839 정확도 : 0.6295[1,1]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:학습 Loss : 1.5406 정확도 : 0.2991\n[1,0]&lt;stdout&gt;:검증 Loss : 1.3772 정확도 : 0.4219[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 8/10[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:----------[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:검증 Loss : 1.3772 정확도 : 0.4219[1,1]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:Epoch 8/10\n[1,1]&lt;stdout&gt;:----------\n[1,1]&lt;stdout&gt;:학습 Loss : 1.2026 정확도 : 0.6071[1,1]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:학습 Loss : 1.4964 정확도 : 0.3170\n[1,0]&lt;stdout&gt;:검증 Loss : 1.4477 정확도 : 0.4062[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 9/10[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:----------[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:검증 Loss : 1.4477 정확도 : 0.4062\n[1,1]&lt;stdout&gt;:Epoch 9/10\n[1,1]&lt;stdout&gt;:----------\n[1,0]&lt;stdout&gt;:학습 Loss : 1.5409 정확도 : 0.2969[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:학습 Loss : 1.1575 정확도 : 0.6362[1,1]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:검증 Loss : 1.4076 정확도 : 0.4062[1,1]&lt;stdout&gt;:검증 Loss : 1.4076 정확도 : 0.4062\n[1,1]&lt;stdout&gt;:Epoch 10/10[1,1]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:----------\n[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 10/10\n[1,0]&lt;stdout&gt;:----------[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:학습 Loss : 1.1559 정확도 : 0.6362[1,1]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:학습 Loss : 1.4953 정확도 : 0.3326[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:검증 Loss : 1.4324 정확도 : 0.4062\n[1,0]&lt;stdout&gt;:검증 Loss : 1.4324 정확도 : 0.4062[1,0]&lt;stdout&gt;:\nOut[73]: 1.4323782920837402</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">HorovodRunner will stream all training logs to notebook cell output. If there are too many logs, you\ncan adjust the log level in your train method. Or you can set driver_log_verbosity to\n&#39;log_callback_only&#39; and use a HorovodRunner log  callback on the first worker to get concise\nprogress updates.\nThe global names read or written to by the pickled function are {&#39;range&#39;, &#39;converter_val&#39;, &#39;hvd&#39;, &#39;NUM_EPOCHS&#39;, &#39;evaluate&#39;, &#39;get_transform_spec&#39;, &#39;iter&#39;, &#39;len&#39;, &#39;converter_train&#39;, &#39;torch&#39;, &#39;get_model&#39;, &#39;BATCH_SIZE&#39;, &#39;print&#39;, &#39;metric_average&#39;, &#39;max&#39;, &#39;train_one_epoch&#39;}.\nThe pickled object size is 7863 bytes.\n\n### How to enable Horovod Timeline? ###\nHorovodRunner has the ability to record the timeline of its activity with Horovod  Timeline. To\nrecord a Horovod Timeline, set the `HOROVOD_TIMELINE` environment variable  to the location of the\ntimeline file to be created. You can then open the timeline file  using the chrome://tracing\nfacility of the Chrome browser.\n\nStart training.\n[1,0]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,0]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,1]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,1]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,0]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,0]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,0]&lt;stdout&gt;:Epoch 1/10[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:----------[1,0]&lt;stdout&gt;:\n[1,1]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,1]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,1]&lt;stdout&gt;:Epoch 1/10\n[1,1]&lt;stdout&gt;:----------\n[1,1]&lt;stderr&gt;:/databricks/python/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py:53: FutureWarning: Calling .data on ChunkedArray is provided for compatibility after Column was removed, simply drop this attribute\n[1,1]&lt;stderr&gt;:  column_as_pandas = column.data.chunks[0].to_pandas()\n[1,0]&lt;stderr&gt;:/databricks/python/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py:53: FutureWarning: Calling .data on ChunkedArray is provided for compatibility after Column was removed, simply drop this attribute\n[1,0]&lt;stderr&gt;:  column_as_pandas = column.data.chunks[0].to_pandas()\n[1,0]&lt;stdout&gt;:학습 Loss : 3.0049 정확도 : 0.2991\n[1,1]&lt;stdout&gt;:학습 Loss : 2.9046 정확도 : 0.4464\n[1,1]&lt;stderr&gt;:-c:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n[1,0]&lt;stderr&gt;:-c:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n[1,1]&lt;stdout&gt;:검증 Loss : 1.4322 정확도 : 0.3438\n[1,0]&lt;stdout&gt;:검증 Loss : 1.4322 정확도 : 0.3438\n[1,0]&lt;stdout&gt;:Epoch 2/10\n[1,0]&lt;stdout&gt;:----------\n[1,1]&lt;stdout&gt;:Epoch 2/10\n[1,1]&lt;stdout&gt;:----------\n[1,0]&lt;stdout&gt;:학습 Loss : 1.4913 정확도 : 0.2991[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:학습 Loss : 1.2598 정확도 : 0.5268\n[1,1]&lt;stdout&gt;:검증 Loss : 1.3130 정확도 : 0.3906\n[1,1]&lt;stdout&gt;:Epoch 3/10\n[1,1]&lt;stdout&gt;:----------\n[1,0]&lt;stdout&gt;:검증 Loss : 1.3130 정확도 : 0.3906[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 3/10[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:----------[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:학습 Loss : 1.1122 정확도 : 0.6406\n[1,0]&lt;stdout&gt;:학습 Loss : 1.5724 정확도 : 0.2902\n[1,1]&lt;stdout&gt;:검증 Loss : 1.5220 정확도 : 0.3750\n[1,1]&lt;stdout&gt;:Epoch 4/10\n[1,1]&lt;stdout&gt;:----------\n[1,0]&lt;stdout&gt;:검증 Loss : 1.5220 정확도 : 0.3750[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 4/10[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:----------[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:학습 Loss : 1.6128 정확도 : 0.3080\n[1,1]&lt;stdout&gt;:학습 Loss : 0.9855 정확도 : 0.6183\n[1,0]&lt;stdout&gt;:검증 Loss : 1.4703 정확도 : 0.3438\n[1,0]&lt;stdout&gt;:Epoch 5/10\n[1,0]&lt;stdout&gt;:----------[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:검증 Loss : 1.4703 정확도 : 0.3438\n[1,1]&lt;stdout&gt;:Epoch 5/10\n[1,1]&lt;stdout&gt;:----------[1,1]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:학습 Loss : 1.1860 정확도 : 0.6228\n[1,0]&lt;stdout&gt;:학습 Loss : 1.5704 정확도 : 0.2969[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:검증 Loss : 1.4714 정확도 : 0.3750\n[1,1]&lt;stdout&gt;:Epoch 6/10\n[1,1]&lt;stdout&gt;:----------\n[1,0]&lt;stdout&gt;:검증 Loss : 1.4714 정확도 : 0.3750[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 6/10[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:----------[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:학습 Loss : 1.2572 정확도 : 0.6205[1,1]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:학습 Loss : 1.5042 정확도 : 0.2835\n[1,0]&lt;stdout&gt;:검증 Loss : 1.4258 정확도 : 0.3750[1,1]&lt;stdout&gt;:검증 Loss : 1.4258 정확도 : 0.3750[1,1]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:Epoch 7/10\n[1,1]&lt;stdout&gt;:----------\n[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 7/10[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:----------[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:학습 Loss : 1.1839 정확도 : 0.6295[1,1]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:학습 Loss : 1.5406 정확도 : 0.2991\n[1,0]&lt;stdout&gt;:검증 Loss : 1.3772 정확도 : 0.4219[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 8/10[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:----------[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:검증 Loss : 1.3772 정확도 : 0.4219[1,1]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:Epoch 8/10\n[1,1]&lt;stdout&gt;:----------\n[1,1]&lt;stdout&gt;:학습 Loss : 1.2026 정확도 : 0.6071[1,1]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:학습 Loss : 1.4964 정확도 : 0.3170\n[1,0]&lt;stdout&gt;:검증 Loss : 1.4477 정확도 : 0.4062[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 9/10[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:----------[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:검증 Loss : 1.4477 정확도 : 0.4062\n[1,1]&lt;stdout&gt;:Epoch 9/10\n[1,1]&lt;stdout&gt;:----------\n[1,0]&lt;stdout&gt;:학습 Loss : 1.5409 정확도 : 0.2969[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:학습 Loss : 1.1575 정확도 : 0.6362[1,1]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:검증 Loss : 1.4076 정확도 : 0.4062[1,1]&lt;stdout&gt;:검증 Loss : 1.4076 정확도 : 0.4062\n[1,1]&lt;stdout&gt;:Epoch 10/10[1,1]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:----------\n[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 10/10\n[1,0]&lt;stdout&gt;:----------[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:학습 Loss : 1.1559 정확도 : 0.6362[1,1]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:학습 Loss : 1.4953 정확도 : 0.3326[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:검증 Loss : 1.4324 정확도 : 0.4062\n[1,0]&lt;stdout&gt;:검증 Loss : 1.4324 정확도 : 0.4062[1,0]&lt;stdout&gt;:\nOut[73]: 1.4323782920837402</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0185b736-a17f-4344-9bd9-9449f7fe4cf8"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"petastorm-spark-converter-pytorch","dashboards":[],"language":"python","widgets":{},"notebookOrigID":4227274535566174}},"nbformat":4,"nbformat_minor":0}
